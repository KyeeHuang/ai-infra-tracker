[
  {
    "id": 4,
    "name": "DeepSeek-V3",
    "full_name": "deepseek-ai/DeepSeek-V3",
    "description": null,
    "url": "https://github.com/deepseek-ai/DeepSeek-V3",
    "stars": 101368,
    "forks": 16494,
    "open_issues": 91,
    "watchers": 101368,
    "language": "Python",
    "license": "MIT",
    "updated_at": "2026-01-29T02:31:09Z",
    "topics": "",
    "scraped_at": "2026-01-29 02:34:23"
  },
  {
    "id": 1,
    "name": "vllm",
    "full_name": "vllm-project/vllm",
    "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
    "url": "https://github.com/vllm-project/vllm",
    "stars": 68891,
    "forks": 12988,
    "open_issues": 3187,
    "watchers": 68891,
    "language": "Python",
    "license": "Apache-2.0",
    "updated_at": "2026-01-29T02:26:14Z",
    "topics": "amd,blackwell,cuda,deepseek,deepseek-v3,gpt,gpt-oss,inference,kimi,llama,llm,llm-serving,model-serving,moe,openai,pytorch,qwen,qwen3,tpu,transformer",
    "scraped_at": "2026-01-29 02:34:18"
  },
  {
    "id": 7,
    "name": "llama",
    "full_name": "meta-llama/llama",
    "description": "Inference code for Llama models",
    "url": "https://github.com/meta-llama/llama",
    "stars": 59089,
    "forks": 9823,
    "open_issues": 512,
    "watchers": 59089,
    "language": "Python",
    "license": "NOASSERTION",
    "updated_at": "2026-01-28T19:31:28Z",
    "topics": "",
    "scraped_at": "2026-01-29 02:34:31"
  },
  {
    "id": 6,
    "name": "DeepSpeed",
    "full_name": "deepspeedai/DeepSpeed",
    "description": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.",
    "url": "https://github.com/deepspeedai/DeepSpeed",
    "stars": 41450,
    "forks": 4700,
    "open_issues": 1262,
    "watchers": 41450,
    "language": "Python",
    "license": "Apache-2.0",
    "updated_at": "2026-01-28T17:00:42Z",
    "topics": "billion-parameters,compression,data-parallelism,deep-learning,gpu,inference,machine-learning,mixture-of-experts,model-parallelism,pipeline-parallelism,pytorch,trillion-parameters,zero",
    "scraped_at": "2026-01-29 02:34:29"
  },
  {
    "id": 5,
    "name": "ColossalAI",
    "full_name": "hpcaitech/ColossalAI",
    "description": "Making large AI models cheaper, faster and more accessible",
    "url": "https://github.com/hpcaitech/ColossalAI",
    "stars": 41331,
    "forks": 4537,
    "open_issues": 489,
    "watchers": 41331,
    "language": "Python",
    "license": "Apache-2.0",
    "updated_at": "2026-01-28T15:16:58Z",
    "topics": "ai,big-model,data-parallelism,deep-learning,distributed-computing,foundation-models,heterogeneous-training,hpc,inference,large-scale,model-parallelism,pipeline-parallelism",
    "scraped_at": "2026-01-29 02:34:26"
  },
  {
    "id": 2,
    "name": "sglang",
    "full_name": "sgl-project/sglang",
    "description": "SGLang is a high-performance serving framework for large language models and multimodal models.",
    "url": "https://github.com/sgl-project/sglang",
    "stars": 22929,
    "forks": 4237,
    "open_issues": 2105,
    "watchers": 22929,
    "language": "Python",
    "license": "Apache-2.0",
    "updated_at": "2026-01-29T02:22:30Z",
    "topics": "attention,blackwell,cuda,deepseek,diffusion,glm,gpt-oss,inference,llama,llm,minimax,moe,qwen,qwen-image,reinforcement-learning,transformer,vlm,wan",
    "scraped_at": "2026-01-29 02:34:19"
  },
  {
    "id": 8,
    "name": "Qwen",
    "full_name": "QwenLM/Qwen",
    "description": "The official repo of Qwen (通义千问) chat & pretrained large language model proposed by Alibaba Cloud.",
    "url": "https://github.com/QwenLM/Qwen",
    "stars": 20263,
    "forks": 1699,
    "open_issues": 25,
    "watchers": 20263,
    "language": "Python",
    "license": "Apache-2.0",
    "updated_at": "2026-01-28T22:05:55Z",
    "topics": "chinese,flash-attention,large-language-models,llm,natural-language-processing,pretrained-models",
    "scraped_at": "2026-01-29 02:34:32"
  },
  {
    "id": 9,
    "name": "ChatGLM3",
    "full_name": "zai-org/ChatGLM3",
    "description": "ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型",
    "url": "https://github.com/zai-org/ChatGLM3",
    "stars": 13761,
    "forks": 1615,
    "open_issues": 32,
    "watchers": 13761,
    "language": "Python",
    "license": "Apache-2.0",
    "updated_at": "2026-01-29T01:07:50Z",
    "topics": "",
    "scraped_at": "2026-01-29 02:34:36"
  },
  {
    "id": 3,
    "name": "TensorRT-LLM",
    "full_name": "NVIDIA/TensorRT-LLM",
    "description": "TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in a performant way.",
    "url": "https://github.com/NVIDIA/TensorRT-LLM",
    "stars": 12754,
    "forks": 2053,
    "open_issues": 979,
    "watchers": 12754,
    "language": "Python",
    "license": "NOASSERTION",
    "updated_at": "2026-01-29T01:54:19Z",
    "topics": "blackwell,cuda,llm-serving,moe,pytorch",
    "scraped_at": "2026-01-29 02:34:21"
  }
]